{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fced008-0250-43c5-aa2b-6db288515e76",
   "metadata": {},
   "source": [
    "In scikit-learn, the RandomSearchCV function implements random search with cross-validation. RandomSearchCV requires three arguments to be specified:\n",
    "\n",
    "- estimator: the machine learning model whose hyperparameters we’re tuning; this is exactly the same for GridSearchCV\n",
    "- param_distributions: a dictionary which specifies the hyperparameters as keys and corresponding distributions to draw lists of values from for each hyperparameter. In GridSearchCV, we instead had param_grid, a dictionary representing the grid of hyperparameters to search from\n",
    "- n_iter: the number of times the algorithm needs to randomly draw from the distributions. The default value for this is 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bb84e3a-068f-49c1-a89f-625f9b8fb263",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "\n",
    "# Load the data set\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303be0a7-ed58-45b1-ba8a-2f826bf2c2dd",
   "metadata": {},
   "source": [
    "provide some information about how we want to select random numbers. Do we want random numbers between 0 and 100? Between -10 and 10? Do we want the same chance of picking small numbers and picking large numbers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd1dbace-b46f-424d-be42-2d0b75cc7a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import uniform\n",
    "distributions = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': uniform(loc=0, scale=100)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfa004c3-e377-4bf1-ada9-eda98be326c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV(estimator=LogisticRegression(max_iter=1000,\n",
      "                                                solver='liblinear'),\n",
      "                   n_iter=8,\n",
      "                   param_distributions={'C': <scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x115f78850>,\n",
      "                                        'penalty': ['l1', 'l2']})\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(solver='liblinear', max_iter=1000)\n",
    "clf = RandomizedSearchCV(lr, distributions,n_iter=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82729c0c-957b-46cd-a03f-2ebf8f725b1a",
   "metadata": {},
   "source": [
    "## Evaluating the Results of `RandomizedSearchCV`\n",
    "\n",
    "We can now follow a similar process to what we did with GridSearchCV to evaluate the results of RandomizedSearchCV.\n",
    "\n",
    "After fitting a RandomizedSearchCV model we can find out the results using the following attributes of the clf argument:\n",
    "\n",
    "- .best_estimator_ gives us the best estimator\n",
    "- .best_score_ gives us the mean cross-validated score corresponding to the best estimator\n",
    "- .best_params_ gives us the set of hyperparameters that correspond to the best estimator\n",
    "Additionally, the `.cv_results_ attribute` gives us the scores for each hyperparameter combination in the grid. We’re now ready to evaluate the random search we set up earlier and we’ve preloaded the code from the previous exercise in the Setup cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8aef2c5e-8151-4526-a08f-bd8d15eed331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=np.float64(54.289379228449874), max_iter=1000,\n",
      "                   penalty='l1', solver='liblinear')\n",
      "{'C': np.float64(54.289379228449874), 'penalty': 'l1'}\n"
     ]
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "best_model = clf.best_estimator_\n",
    "\n",
    "print(best_model)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f237098-1193-433e-bf93-d533c313d1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9672229822161423\n",
      "0.965034965034965\n"
     ]
    }
   ],
   "source": [
    "## YOUR SOLUTION HERE ##\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Store the best score from the grid search\n",
    "best_score = clf.best_score_\n",
    "\n",
    "# Get the best estimator and evaluate on the test set\n",
    "best_model = clf.best_estimator_\n",
    "test_predictions = best_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "test_score = accuracy_score(y_test, test_predictions)\n",
    "\n",
    "\n",
    "print(best_score)\n",
    "print(test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0802c6e9-749f-4772-82c6-d8301a241d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           C penalty         0\n",
      "0  87.942761      l2  0.948427\n",
      "1  93.273480      l1  0.955458\n",
      "2  13.126674      l1  0.957811\n",
      "3  54.289379      l1  0.967223\n",
      "4  70.815615      l1  0.957811\n",
      "5  96.889236      l2  0.950780\n",
      "6   2.208408      l1  0.946074\n",
      "7  55.422794      l1  0.962517\n"
     ]
    }
   ],
   "source": [
    "hyperparameter_values = pd.DataFrame(clf.cv_results_['params'])\n",
    "randomsearch_scores = pd.DataFrame(clf.cv_results_['mean_test_score'])\n",
    "\n",
    "\n",
    "df = pd.concat([hyperparameter_values, randomsearch_scores], axis = 1)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696cb9eb-b606-4d84-8cc4-aa01e6877908",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
